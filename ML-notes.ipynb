{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load CSV Using Python Standard Library\n",
    "import csv\n",
    "import numpy\n",
    "filename = ' pima-indians-diabetes.data.csv '\n",
    "raw_data = open(filename, ' rb ' )\n",
    "reader = csv.reader(raw_data, delimiter= ' , ' , quoting=csv.QUOTE_NONE)\n",
    "x = list(reader)\n",
    "data = numpy.array(x).astype( ' float ' )\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load CSV using NumPy\n",
    "from numpy import loadtxt\n",
    "filename = ' pima-indians-diabetes.data.csv '\n",
    "raw_data = open(filename, ' rb ' )\n",
    "data = loadtxt(raw_data, delimiter=\",\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load CSV from URL using NumPy\n",
    "from numpy import loadtxt\n",
    "from urllib import urlopen\n",
    "url = ' https://goo.gl/vhm1eU '\n",
    "raw_data = urlopen(url)\n",
    "dataset = loadtxt(raw_data, delimiter=\",\")\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load CSV using Pandas\n",
    "from pandas import read_csv\n",
    "filename = ' pima-indians-diabetes.data.csv '\n",
    "names = [ ' preg ' , ' plas ' , ' pres ' , ' skin ' , ' test ' , ' mass ' , ' pedi ' , ' age ' , ' class ' ]\n",
    "data = read_csv(filename, names=names)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Types for Each Attribute\n",
    "from pandas import read_csv\n",
    "filename = \"pima-indians-diabetes.data.csv\"\n",
    "names = [ ' preg ' , ' plas ' , ' pres ' , ' skin ' , ' test ' , ' mass ' , ' pedi ' , ' age ' , ' class ' ]\n",
    "data = read_csv(filename, names=names)\n",
    "types = data.dtypes\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Statistical Summary\n",
    "from pandas import read_csv\n",
    "from pandas import set_option\n",
    "filename = \"pima-indians-diabetes.data.csv\"\n",
    "names = [ ' preg ' , ' plas ' , ' pres ' , ' skin ' , ' test ' , ' mass ' , ' pedi ' , ' age ' , ' class ' ]\n",
    "data = read_csv(filename, names=names)\n",
    "set_option( ' display.width ' , 100)\n",
    "set_option( ' precision ' , 3)\n",
    "description = data.describe()\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output for above\n",
    "preg\n",
    "plas\n",
    "pres\n",
    "skin\n",
    "test\n",
    "mass\n",
    "pedi\n",
    "age\n",
    "class\n",
    "count 768.000 768.000 768.000 768.000 768.000 768.000 768.000 768.000 768.000\n",
    "mean\n",
    "3.845 120.895 69.105 20.536 79.799 31.993\n",
    "0.472 33.241\n",
    "0.349\n",
    "std\n",
    "3.370 31.973 19.356 15.952 115.244\n",
    "7.884\n",
    "0.331 11.760\n",
    "0.477\n",
    "min\n",
    "0.000\n",
    "0.000\n",
    "0.000\n",
    "0.000\n",
    "0.000\n",
    "0.000\n",
    "0.078 21.000\n",
    "0.000\n",
    "25%\n",
    "1.000 99.000 62.000\n",
    "0.000\n",
    "0.000 27.300\n",
    "0.244 24.000\n",
    "0.000\n",
    "50%\n",
    "3.000 117.000 72.000 23.000 30.500 32.000\n",
    "0.372 29.000\n",
    "0.000\n",
    "75%\n",
    "6.000 140.250 80.000 32.000 127.250 36.600\n",
    "0.626 41.000\n",
    "1.000\n",
    "max\n",
    "17.000 199.000 122.000 99.000 846.000 67.100\n",
    "2.420 81.000\n",
    "1.000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preg\n",
    "plas\n",
    "pres\n",
    "skin\n",
    "test\n",
    "mass\n",
    "pedi\n",
    "age\n",
    "class\n",
    "\n",
    "count \n",
    "768.000 768.000 768.000 768.000 768.000 768.000 768.000 768.000 768.000\n",
    "mean\n",
    "3.845 120.895 69.105 20.536 79.799 31.993\n",
    "0.472 33.241\n",
    "0.349\n",
    "\n",
    "std\n",
    "3.370 31.973 19.356 15.952 115.244\n",
    "7.884\n",
    "0.331 11.760\n",
    "0.477\n",
    "\n",
    "min\n",
    "0.000\n",
    "0.000\n",
    "0.000\n",
    "0.000\n",
    "0.000\n",
    "0.000\n",
    "0.078 21.000\n",
    "0.000\n",
    "\n",
    "25%\n",
    "1.000 99.000 62.000\n",
    "0.000\n",
    "0.000 27.300\n",
    "0.244 24.000\n",
    "0.000\n",
    "\n",
    "50%\n",
    "3.000 117.000 72.000 23.000 30.500 32.000\n",
    "0.372 29.000\n",
    "0.000\n",
    "\n",
    "75%\n",
    "6.000 140.250 80.000 32.000 127.250 36.600\n",
    "0.626 41.000\n",
    "1.000\n",
    "\n",
    "max\n",
    "17.000 199.000 122.000 99.000 846.000 67.100\n",
    "2.420 81.000\n",
    "1.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class Distribution\n",
    "class_counts = data.groupby( ' class ' ).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Correlations Between Attributes\n",
    "\n",
    "Correlation refers to the relationship between two variables and how they may or may not\n",
    "change together. \n",
    "\n",
    "The most common method for calculating correlation is Pearsonâ€™s Correlation\n",
    "Coefficient, that assumes a normal distribution of the attributes involved. A correlation of -1\n",
    "or 1 shows a full negative or positive correlation respectively. Whereas a value of 0 shows no\n",
    "correlation at all. Some machine learning algorithms like linear and logistic regression can suffer\n",
    "poor performance if there are highly correlated attributes in your dataset. As such, it is a good\n",
    "idea to review all of the pairwise correlations of the attributes in your dataset. You can use the\n",
    "corr() function on the Pandas DataFrame to calculate a correlation matrix.\n",
    "\n",
    "correlations = data.corr(method= ' pearson ' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Skew of Univariate Distributions\n",
    "\n",
    "Skew refers to a distribution that is assumed Gaussian (normal or bell curve) that is shifted or\n",
    "squashed in one direction or another. \n",
    "\n",
    "Many machine learning algorithms assume a Gaussian\n",
    "distribution. Knowing that an attribute has a skew may allow you to perform data preparation\n",
    "to correct the skew and later improve the accuracy of your models. You can calculate the skew\n",
    "of each attribute using the skew() function on the Pandas DataFrame.\n",
    "\n",
    "skew = data.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Histogram:\n",
    "data.hist()\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Density plots:\n",
    "data.plot(kind= ' density ' , subplots=True, layout=(3,3), sharex=False)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Box and Whisker Plots\n",
    "Boxplots summarize the distribution of each attribute, drawing a line for\n",
    "the median (middle value) and a box around the 25th and 75th percentiles (the middle 50% of\n",
    "the data). \n",
    "\n",
    "The whiskers give an idea of the spread of the data and dots outside of the whiskers\n",
    "show candidate outlier values (values that are 1.5 times greater than the size of spread of the\n",
    "middle 50% of the data).\n",
    "\n",
    "# Box and Whisker Plots\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "filename = \"pima-indians-diabetes.data.csv\"\n",
    "names = [ ' preg ' , ' plas ' , ' pres ' , ' skin ' , ' test ' , ' mass ' , ' pedi ' , ' age ' , ' class ' ]\n",
    "data = read_csv(filename, names=names)\n",
    "data.plot(kind= ' box ' , subplots=True, layout=(3,3), sharex=False, sharey=False)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Correction Matrix Plot (generic)\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "import numpy\n",
    "filename = ' pima-indians-diabetes.data.csv '\n",
    "names = [ ' preg ' , ' plas ' , ' pres ' , ' skin ' , ' test ' , ' mass ' , ' pedi ' , ' age ' , ' class ' ]\n",
    "data = read_csv(filename, names=names)\n",
    "correlations = data.corr()\n",
    "# plot correlation matrix\n",
    "fig = pyplot.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Bias - Variance\n",
    "\n",
    "High Bias - under fitting - not sensitive to changes in data at all\n",
    "High Variance - over fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling of data\n",
    "> Rescale data.\n",
    "> Standardize data.\n",
    "> Normalize data.\n",
    "> Binarize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Rescale data:\n",
    "    \n",
    "When your data is comprised of attributes with varying scales, many machine learning algorithms\n",
    "can benefit from rescaling the attributes to all have the same scale. Often this is referred to\n",
    "as normalization and attributes are often rescaled into the range between 0 and 1.\n",
    "\n",
    "You can rescale your data\n",
    "using scikit-learn using the MinMaxScaler class 2 .\n",
    "\n",
    "Ex:\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Standardization\n",
    "\n",
    "Standardization is a useful technique to transform attributes with a Gaussian distribution and\n",
    "differing means and standard deviations to a standard Gaussian distribution with a mean of\n",
    "0 and a standard deviation of 1. \n",
    "\n",
    "It is most suitable for techniques that assume a Gaussian\n",
    "distribution in the input variables and work better with rescaled data, such as linear regression,\n",
    "logistic regression and linear discriminate analysis. \n",
    "\n",
    "You can standardize data using scikit-learn with the StandardScaler class 3 .\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Normalizing\n",
    "\n",
    "Normalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called\n",
    "a unit norm or a vector with the length of 1 in linear algebra). This pre-processing method\n",
    "can be useful for sparse datasets (lots of zeros) with attributes of varying scales when using\n",
    "algorithms that weight input values such as neural networks and algorithms that use distance\n",
    "measures such as k-Nearest Neighbors. \n",
    "\n",
    "You can normalize data in Python with scikit-learn\n",
    "using the Normalizer class 4 .\n",
    "\n",
    "scaler = Normalizer()\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "\n",
    ">>> X = [[ 1., -1.,  2.],\n",
    "...      [ 2.,  0.,  0.],\n",
    "...      [ 0.,  1., -1.]]\n",
    ">>> X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "\n",
    ">>> X_normalized                                      \n",
    "array([[ 0.40..., -0.40...,  0.81...],\n",
    "       [ 1.  ...,  0.  ...,  0.  ...],\n",
    "       [ 0.  ...,  0.70..., -0.70...]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Binarize Data (Make Binary)\n",
    "\n",
    "You can transform your data using a binary threshold. All values above the threshold are\n",
    "marked 1 and all equal to or below are marked as 0.\n",
    "\n",
    "binarizer = Binarizer(threshold=0.0).fit(X)\n",
    "binaryX = binarizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feature Selection For Machine Learning\n",
    "\n",
    "4 different automatic feature selection techniques:\n",
    "> Univariate Selection.\n",
    "> Recursive Feature Elimination.\n",
    "> Principle Component Analysis.\n",
    "> Feature Importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Statistical tests can be used to select those features that have the strongest relationship with\n",
    "the output variable. \n",
    "\n",
    "The scikit-learn library provides the SelectKBest class that can be used\n",
    "with a suite of different statistical tests to select a specific number of features.\n",
    "\n",
    "The example below uses the chi-squared (chi 2 ) statistical test for non-negative features to select 4 of the best\n",
    "features from the Pima Indians onset of diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "test = SelectKBest(score_func=chi2, k=4) # 4 features\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# summarize scores\n",
    "set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "\n",
    "# summarize selected features\n",
    "print(features[0:5,:]) # will contain only 4 columns (k=4)\n",
    "\n",
    "or]\n",
    "features = SelectKBest(score_func=chi2,k=4).fit(X,Y).transform(X)\n",
    "print (features[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Recursive Feature Elimination:\n",
    "    \n",
    "The Recursive Feature Elimination (or RFE) works by recursively removing attributes and\n",
    "building a model on those attributes that remain. \n",
    "\n",
    "It uses the model accuracy to identify which\n",
    "attributes (and combination of attributes) contribute the most to predicting the target attribute.\n",
    "\n",
    "The example below uses RFE with the logistic regression algorithm to select the top 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The Recursive Feature Elimination\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model,3) #number of features = 3\n",
    "\n",
    "fit = rfe.fit(X,Y)\n",
    "print(\"Num Features: %d\") % fit.n_features_\n",
    "print(\"Selected Features: %s\") % fit.support_\n",
    "print(\"Feature Ranking: %s\") % fit.ranking_\n",
    "\n",
    "Output:\n",
    "Num Features: 3\n",
    "Selected Features: [ True False False False False True True False]\n",
    "Feature Ranking: [1 2 3 5 6 1 1 4]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Principal Component Analysis (or PCA):\n",
    "Uses linear algebra to transform the dataset into a compressed form. Generally this is called a data reduction technique. A property of PCA is that\n",
    "you can choose the number of dimensions or principal components in the transformed result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PCA feature extraction\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "# summarize components\n",
    "print(\"Explained Variance: %s\") % fit.explained_variance_ratio_\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Feature Importance\n",
    "\n",
    "Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance\n",
    "of features. \n",
    "\n",
    "In the example below we construct a ExtraTreesClassifier classifier for the Pima\n",
    "Indians onset of diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "print(model.feature_importances_)\n",
    "\n",
    "#Output:\n",
    "    [ 0.11070069 0.2213717 0.08824115 0.08068703 0.07281761 0.14548537 0.12654214 0.15415431]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation techniques:\n",
    "> Train and Test Sets.\n",
    "\n",
    "> k-fold Cross Validation.\n",
    "\n",
    "> Leave One Out Cross Validation.\n",
    "\n",
    "> Repeated Random Test-Train Splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "> Train and Test Sets.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33,\n",
    "random_state=seed)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "> K-fold:\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0)\n",
    "\n",
    "\n",
    "> Leave one out:\n",
    "loocv = LeaveOneOut()\n",
    "\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=loocv)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0)\n",
    "\n",
    "> Repeated Random Test-Train Splits:\n",
    "kfold = ShuffleSplit(n_splits=10, test_size=0.33, random_state=seed)\n",
    "\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics:\n",
    "\n",
    "> Classification Accuracy.\n",
    "\n",
    "> Logarithmic Loss.\n",
    "\n",
    "> Area Under ROC Curve.\n",
    "\n",
    "> Confusion Matrix.\n",
    "\n",
    "> Classification Report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "> Classification Accuracy.\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LogisticRegression()\n",
    "\n",
    "scoring = ' accuracy '\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"Accuracy: %.3f (%.3f)\") % (results.mean(), results.std())\n",
    "\n",
    "> Logarithmic Loss.\n",
    "scoring = ' neg_log_loss '\n",
    "\n",
    "> Area Under ROC Curve:\n",
    "scoring = ' roc_auc '\n",
    "\n",
    "Sensitivity is the true positive rate also called the recall. It is the number of instances\n",
    "from the positive (first) class that actually predicted correctly.\n",
    "\n",
    "Specificity is also called the true negative rate. Is the number of instances from the\n",
    "negative (second) class that were actually predicted correctly.\n",
    "\n",
    "ex: If AUC is 0.8 - relatively close to 1 and greater than 0.5, suggesting some skill in the predictions\n",
    "\n",
    "> Confusion Matrix.\n",
    "\n",
    "The table presents predictions on the x-axis and accuracy outcomes on the y-axis.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\n",
    "random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "matrix = confusion_matrix(Y_test, predicted)\n",
    "\n",
    "\n",
    "> Classification Report.\n",
    "\n",
    "Quick idea of the accuracy of a model using a number of measure like precision, recall, F1-score and support for each\n",
    "class.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\n",
    "random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "report = classification_report(Y_test, predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Metrics:\n",
    "    \n",
    "> Mean Absolute Error.\n",
    "\n",
    "> Mean Squared Error.\n",
    "\n",
    "> R squared/coefficient of determination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "> Mean Absolute Error.\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LinearRegression()\n",
    "scoring = ' neg_mean_absolute_error '\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\n",
    "A value of 0 indicates no error or perfect predictions\n",
    "\n",
    "> Mean Squared Error.\n",
    "scoring = ' neg_mean_squared_error '\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\n",
    "> R squared/coefficient of determination.\n",
    "scoring = ' r2 '\n",
    "\n",
    "This is a value between 0 and 1 for no-fit and perfect fit respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Six classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Linear learning algorithms:\n",
    "    \n",
    "1. Logistic Regression.\n",
    "2. Linear Discriminant Analysis.\n",
    "\n",
    "Nonlinear learning algorithms:\n",
    "3. k-Nearest Neighbors.\n",
    "4. Naive Bayes.\n",
    "5. Classification and Regression Trees.\n",
    "6. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linear regression vs Logistic regression\n",
    "\n",
    "In linear regression, the outcome (dependent variable) is continuous. It can have any one of an infinite number of possible values. \n",
    "\n",
    "In logistic regression, the outcome (dependent variable) has only a limited number of possible values.\n",
    "\n",
    "For instance, if X contains the area in square feet of houses, and Y contains the corresponding sale price of those houses, you could use linear regression to predict selling price as a function of house size. While the possible selling price may not actually be any, there are so many possible values that a linear regression model would be chosen.\n",
    "\n",
    "If, instead, you wanted to predict, based on size, whether a house would sell for more than 200K, you would use logistic regression. The possible outputs are either Yes, the house will sell for more than 200K, or No, the house will not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does linear mean?\n",
    "\n",
    "Linear regression requires a linear model. No surprise, right? But what does that really mean?\n",
    "\n",
    "A model is linear when each term is either a constant or the product of a parameter and a predictor variable. A linear equation is constructed by adding the results for each term. This constrains the equation to just one basic form:\n",
    "\n",
    "Response = constant + parameter * predictor + ... + parameter * predictor\n",
    "\n",
    "Y = b o + b1X1 + b2X2 + ... + bkXk\n",
    "\n",
    "While a linear equation has one basic form, nonlinear equations can take many different forms.\n",
    "Literally, itâ€™s not linear. If the equation doesnâ€™t meet the criteria above for a linear equation, itâ€™s nonlinear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regression Algorithms:\n",
    "#### Linear machine learning algorithms:\n",
    "1. Linear Regression.\n",
    "2. Ridge Regression.\n",
    "3. LASSO Linear Regression.\n",
    "4. Elastic Net Regression.\n",
    "\n",
    "#### Nonlinear machine learning algorithms:\n",
    "5. k-Nearest Neighbors.\n",
    "6. Classification and Regression Trees.\n",
    "7. Support Vector Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Classification algorithms:\n",
    "\n",
    "#### Linear algorithms\n",
    "1. Logistic Regression \n",
    "2. Linear Discriminant Analysis\n",
    "   \n",
    "#### Nonlinear algorithms:\n",
    "3. k-Nearest Neighbors\n",
    "4. Naive Bayes\n",
    "5. Classification and Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_svm=SVC()\n",
    "results_svm=cross_val_score(model_svm,X,Y,cv=kfold)\n",
    "print (\"SVM: %s\" % results_svm.mean())\n",
    "SVM: 0.651025290499\n",
    "\n",
    "model_knn = KNeighborsClassifier()\n",
    "results_knn = cross_val_score(model_knn,X,Y,cv=kfold)\n",
    "print (\"KNN: %s\" % results_knn.mean())\n",
    "KNN: 0.726555023923\n",
    "\n",
    "\n",
    "model_decision = DecisionTreeClassifier()\n",
    "results_decision = cross_val_score(model_decision,X,Y,cv=kfold)\n",
    "print (\"Descision Tree: %s\" % results_decision.mean())\n",
    "Descision Tree: 0.691285030759\n",
    "\n",
    "    \n",
    "model_LDA = LinearDiscriminantAnalysis()\n",
    "results_model_LDA = cross_val_score(model_LDA,X,Y,cv=kfold)\n",
    "print (\"LDA: %s\" % results_model_LDA.mean())\n",
    "LDA: 0.773462064252\n",
    "\n",
    "model_LogisticRegression = LogisticRegression()\n",
    "results_LogisticRegression = cross_val_score(model_LogisticRegression,X,Y,cv=kfold)\n",
    "print (\"Logistic Regression: %s\" % results_LogisticRegression.mean())\n",
    "Logistic Regression: 0.76951469583\n",
    "\n",
    "    \n",
    "model_decision = DecisionTreeClassifier()\n",
    "results_decision = cross_val_score(model_decision,X,Y,cv=kfold)\n",
    "print (\"Descision Tree: %s\" % results_decision.mean())\n",
    "Descision Tree: 0.699213943951"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
