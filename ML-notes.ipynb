{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Python Project Template\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Prepare Problem\n",
    "\n",
    "Load libraries\n",
    "Load dataset\n",
    "\n",
    "2. Summarize Data\n",
    "\n",
    "Descriptive statistics\n",
    "Data visualizations\n",
    " \n",
    "3. Prepare Data\n",
    "Data Cleaning\n",
    "Feature Selection\n",
    "Data Transforms\n",
    " \n",
    "4. Evaluate Algorithms\n",
    "Split-out validation dataset\n",
    "Test options and evaluation metric\n",
    "Spot Check Algorithms\n",
    "Compare Algorithms\n",
    "  \n",
    "5. Improve Accuracy\n",
    "Algorithm Tuning\n",
    "Ensembles\n",
    " \n",
    "6. Finalize Model\n",
    "Predictions on validation dataset\n",
    "Create standalone model on entire training dataset\n",
    "Save model for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load CSV Using Python Standard Library\n",
    "import csv\n",
    "import numpy\n",
    "filename = ' pima-indians-diabetes.data.csv '\n",
    "raw_data = open(filename, ' rb ' )\n",
    "reader = csv.reader(raw_data, delimiter= ' , ' , quoting=csv.QUOTE_NONE)\n",
    "x = list(reader)\n",
    "data = numpy.array(x).astype( ' float ' )\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load CSV using NumPy\n",
    "from numpy import loadtxt\n",
    "filename = ' pima-indians-diabetes.data.csv '\n",
    "raw_data = open(filename, ' rb ' )\n",
    "data = loadtxt(raw_data, delimiter=\",\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load CSV from URL using NumPy\n",
    "from numpy import loadtxt\n",
    "from urllib import urlopen\n",
    "url = ' https://goo.gl/vhm1eU '\n",
    "raw_data = urlopen(url)\n",
    "dataset = loadtxt(raw_data, delimiter=\",\")\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load CSV using Pandas\n",
    "from pandas import read_csv\n",
    "filename = ' pima-indians-diabetes.data.csv '\n",
    "names = [ ' preg ' , ' plas ' , ' pres ' , ' skin ' , ' test ' , ' mass ' , ' pedi ' , ' age ' , ' class ' ]\n",
    "data = read_csv(filename, names=names)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Types for Each Attribute\n",
    "from pandas import read_csv\n",
    "filename = \"pima-indians-diabetes.data.csv\"\n",
    "names = [ ' preg ' , ' plas ' , ' pres ' , ' skin ' , ' test ' , ' mass ' , ' pedi ' , ' age ' , ' class ' ]\n",
    "data = read_csv(filename, names=names)\n",
    "types = data.dtypes\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Statistical Summary\n",
    "from pandas import read_csv\n",
    "from pandas import set_option\n",
    "filename = \"pima-indians-diabetes.data.csv\"\n",
    "names = [ ' preg ' , ' plas ' , ' pres ' , ' skin ' , ' test ' , ' mass ' , ' pedi ' , ' age ' , ' class ' ]\n",
    "data = read_csv(filename, names=names)\n",
    "set_option( ' display.width ' , 100)\n",
    "set_option( ' precision ' , 3)\n",
    "description = data.describe()\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output for above\n",
    "preg\n",
    "plas\n",
    "pres\n",
    "skin\n",
    "test\n",
    "mass\n",
    "pedi\n",
    "age\n",
    "class\n",
    "count 768.000 768.000 768.000 768.000 768.000 768.000 768.000 768.000 768.000\n",
    "mean\n",
    "3.845 120.895 69.105 20.536 79.799 31.993\n",
    "0.472 33.241\n",
    "0.349\n",
    "std\n",
    "3.370 31.973 19.356 15.952 115.244\n",
    "7.884\n",
    "0.331 11.760\n",
    "0.477\n",
    "min\n",
    "0.000\n",
    "0.000\n",
    "0.000\n",
    "0.000\n",
    "0.000\n",
    "0.000\n",
    "0.078 21.000\n",
    "0.000\n",
    "25%\n",
    "1.000 99.000 62.000\n",
    "0.000\n",
    "0.000 27.300\n",
    "0.244 24.000\n",
    "0.000\n",
    "50%\n",
    "3.000 117.000 72.000 23.000 30.500 32.000\n",
    "0.372 29.000\n",
    "0.000\n",
    "75%\n",
    "6.000 140.250 80.000 32.000 127.250 36.600\n",
    "0.626 41.000\n",
    "1.000\n",
    "max\n",
    "17.000 199.000 122.000 99.000 846.000 67.100\n",
    "2.420 81.000\n",
    "1.000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preg\n",
    "plas\n",
    "pres\n",
    "skin\n",
    "test\n",
    "mass\n",
    "pedi\n",
    "age\n",
    "class\n",
    "\n",
    "count \n",
    "768.000 768.000 768.000 768.000 768.000 768.000 768.000 768.000 768.000\n",
    "mean\n",
    "3.845 120.895 69.105 20.536 79.799 31.993\n",
    "0.472 33.241\n",
    "0.349\n",
    "\n",
    "std\n",
    "3.370 31.973 19.356 15.952 115.244\n",
    "7.884\n",
    "0.331 11.760\n",
    "0.477\n",
    "\n",
    "min\n",
    "0.000\n",
    "0.000\n",
    "0.000\n",
    "0.000\n",
    "0.000\n",
    "0.000\n",
    "0.078 21.000\n",
    "0.000\n",
    "\n",
    "25%\n",
    "1.000 99.000 62.000\n",
    "0.000\n",
    "0.000 27.300\n",
    "0.244 24.000\n",
    "0.000\n",
    "\n",
    "50%\n",
    "3.000 117.000 72.000 23.000 30.500 32.000\n",
    "0.372 29.000\n",
    "0.000\n",
    "\n",
    "75%\n",
    "6.000 140.250 80.000 32.000 127.250 36.600\n",
    "0.626 41.000\n",
    "1.000\n",
    "\n",
    "max\n",
    "17.000 199.000 122.000 99.000 846.000 67.100\n",
    "2.420 81.000\n",
    "1.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class Distribution\n",
    "class_counts = data.groupby( ' class ' ).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Correlations Between Attributes\n",
    "\n",
    "Correlation refers to the relationship between two variables and how they may or may not\n",
    "change together. \n",
    "\n",
    "The most common method for calculating correlation is Pearsonâ€™s Correlation\n",
    "Coefficient, that assumes a normal distribution of the attributes involved. A correlation of -1\n",
    "or 1 shows a full negative or positive correlation respectively. Whereas a value of 0 shows no\n",
    "correlation at all. Some machine learning algorithms like linear and logistic regression can suffer\n",
    "poor performance if there are highly correlated attributes in your dataset. As such, it is a good\n",
    "idea to review all of the pairwise correlations of the attributes in your dataset. You can use the\n",
    "corr() function on the Pandas DataFrame to calculate a correlation matrix.\n",
    "\n",
    "correlations = data.corr(method= ' pearson ' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Skew of Univariate Distributions\n",
    "\n",
    "Skew refers to a distribution that is assumed Gaussian (normal or bell curve) that is shifted or\n",
    "squashed in one direction or another. \n",
    "\n",
    "Many machine learning algorithms assume a Gaussian\n",
    "distribution. Knowing that an attribute has a skew may allow you to perform data preparation\n",
    "to correct the skew and later improve the accuracy of your models. You can calculate the skew\n",
    "of each attribute using the skew() function on the Pandas DataFrame.\n",
    "\n",
    "skew = data.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate plots to better understand each attribute.\n",
    "### Multivariate plots to better understand the relationships between attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# box and whisker plots\n",
    "dataset.plot(kind= ' box ' , subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multivariate plot\n",
    "# scatter plot matrix\n",
    "scatter_matrix(dataset)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Histogram:\n",
    "data.hist()\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Density plots:\n",
    "data.plot(kind= ' density ' , subplots=True, layout=(3,3), sharex=False)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Box and Whisker Plots\n",
    "Boxplots summarize the distribution of each attribute, drawing a line for\n",
    "the median (middle value) and a box around the 25th and 75th percentiles (the middle 50% of\n",
    "the data). \n",
    "\n",
    "The whiskers give an idea of the spread of the data and dots outside of the whiskers\n",
    "show candidate outlier values (values that are 1.5 times greater than the size of spread of the\n",
    "middle 50% of the data).\n",
    "\n",
    "# Box and Whisker Plots\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "filename = \"pima-indians-diabetes.data.csv\"\n",
    "names = [ ' preg ' , ' plas ' , ' pres ' , ' skin ' , ' test ' , ' mass ' , ' pedi ' , ' age ' , ' class ' ]\n",
    "data = read_csv(filename, names=names)\n",
    "data.plot(kind= ' box ' , subplots=True, layout=(3,3), sharex=False, sharey=False)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Correction Matrix Plot (generic)\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "import numpy\n",
    "filename = ' pima-indians-diabetes.data.csv '\n",
    "names = [ ' preg ' , ' plas ' , ' pres ' , ' skin ' , ' test ' , ' mass ' , ' pedi ' , ' age ' , ' class ' ]\n",
    "data = read_csv(filename, names=names)\n",
    "correlations = data.corr()\n",
    "# plot correlation matrix\n",
    "fig = pyplot.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Bias - Variance\n",
    "\n",
    "High Bias - under fitting - not sensitive to changes in data at all\n",
    "High Variance - over fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scaling of data\n",
    "> Rescale data.\n",
    "> Standardize data.\n",
    "> Normalize data.\n",
    "> Binarize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Rescale data:\n",
    "    \n",
    "When your data is comprised of attributes with varying scales, many machine learning algorithms\n",
    "can benefit from rescaling the attributes to all have the same scale. Often this is referred to\n",
    "as normalization and attributes are often rescaled into the range between 0 and 1.\n",
    "\n",
    "You can rescale your data\n",
    "using scikit-learn using the MinMaxScaler class 2 .\n",
    "\n",
    "Ex:\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Standardization\n",
    "\n",
    "Standardization is a useful technique to transform attributes with a Gaussian distribution and\n",
    "differing means and standard deviations to a standard Gaussian distribution with a mean of\n",
    "0 and a standard deviation of 1. \n",
    "\n",
    "It is most suitable for techniques that assume a Gaussian\n",
    "distribution in the input variables and work better with rescaled data, such as linear regression,\n",
    "logistic regression and linear discriminate analysis. \n",
    "\n",
    "You can standardize data using scikit-learn with the StandardScaler class 3 .\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Normalizing\n",
    "\n",
    "Normalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called\n",
    "a unit norm or a vector with the length of 1 in linear algebra). This pre-processing method\n",
    "can be useful for sparse datasets (lots of zeros) with attributes of varying scales when using\n",
    "algorithms that weight input values such as neural networks and algorithms that use distance\n",
    "measures such as k-Nearest Neighbors. \n",
    "\n",
    "You can normalize data in Python with scikit-learn\n",
    "using the Normalizer class 4 .\n",
    "\n",
    "scaler = Normalizer()\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "\n",
    ">>> X = [[ 1., -1.,  2.],\n",
    "...      [ 2.,  0.,  0.],\n",
    "...      [ 0.,  1., -1.]]\n",
    ">>> X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "\n",
    ">>> X_normalized                                      \n",
    "array([[ 0.40..., -0.40...,  0.81...],\n",
    "       [ 1.  ...,  0.  ...,  0.  ...],\n",
    "       [ 0.  ...,  0.70..., -0.70...]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Binarize Data (Make Binary)\n",
    "\n",
    "You can transform your data using a binary threshold. All values above the threshold are\n",
    "marked 1 and all equal to or below are marked as 0.\n",
    "\n",
    "binarizer = Binarizer(threshold=0.0).fit(X)\n",
    "binaryX = binarizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Feature Selection For Machine Learning\n",
    "\n",
    "4 different automatic feature selection techniques:\n",
    "> Univariate Selection.\n",
    "> Recursive Feature Elimination.\n",
    "> Principle Component Analysis.\n",
    "> Feature Importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Statistical tests can be used to select those features that have the strongest relationship with\n",
    "the output variable. \n",
    "\n",
    "The scikit-learn library provides the SelectKBest class that can be used\n",
    "with a suite of different statistical tests to select a specific number of features.\n",
    "\n",
    "The example below uses the chi-squared (chi 2 ) statistical test for non-negative features to select 4 of the best\n",
    "features from the Pima Indians onset of diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "test = SelectKBest(score_func=chi2, k=4) # 4 features\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# summarize scores\n",
    "set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "\n",
    "# summarize selected features\n",
    "print(features[0:5,:]) # will contain only 4 columns (k=4)\n",
    "\n",
    "or]\n",
    "features = SelectKBest(score_func=chi2,k=4).fit(X,Y).transform(X)\n",
    "print (features[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Recursive Feature Elimination:\n",
    "    \n",
    "The Recursive Feature Elimination (or RFE) works by recursively removing attributes and\n",
    "building a model on those attributes that remain. \n",
    "\n",
    "It uses the model accuracy to identify which\n",
    "attributes (and combination of attributes) contribute the most to predicting the target attribute.\n",
    "\n",
    "The example below uses RFE with the logistic regression algorithm to select the top 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The Recursive Feature Elimination\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model,3) #number of features = 3\n",
    "\n",
    "fit = rfe.fit(X,Y)\n",
    "print(\"Num Features: %d\") % fit.n_features_\n",
    "print(\"Selected Features: %s\") % fit.support_\n",
    "print(\"Feature Ranking: %s\") % fit.ranking_\n",
    "\n",
    "Output:\n",
    "Num Features: 3\n",
    "Selected Features: [ True False False False False True True False]\n",
    "Feature Ranking: [1 2 3 5 6 1 1 4]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Principal Component Analysis (or PCA):\n",
    "Uses linear algebra to transform the dataset into a compressed form. Generally this is called a data reduction technique. A property of PCA is that\n",
    "you can choose the number of dimensions or principal components in the transformed result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PCA feature extraction\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "# summarize components\n",
    "print(\"Explained Variance: %s\") % fit.explained_variance_ratio_\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Feature Importance\n",
    "\n",
    "Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance\n",
    "of features. \n",
    "\n",
    "In the example below we construct a ExtraTreesClassifier classifier for the Pima\n",
    "Indians onset of diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "print(model.feature_importances_)\n",
    "\n",
    "#Output:\n",
    "    [ 0.11070069 0.2213717 0.08824115 0.08068703 0.07281761 0.14548537 0.12654214 0.15415431]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  4. Evaluation techniques:\n",
    "> Train and Test Sets.\n",
    "\n",
    "> k-fold Cross Validation.\n",
    "\n",
    "> Leave One Out Cross Validation.\n",
    "\n",
    "> Repeated Random Test-Train Splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "> Train and Test Sets.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33,\n",
    "random_state=seed)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "> K-fold:\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0)\n",
    "\n",
    "\n",
    "> Leave one out:\n",
    "loocv = LeaveOneOut()\n",
    "\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=loocv)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0)\n",
    "\n",
    "> Repeated Random Test-Train Splits:\n",
    "kfold = ShuffleSplit(n_splits=10, test_size=0.33, random_state=seed)\n",
    "\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Classification Metrics:\n",
    "\n",
    "> Classification Accuracy.\n",
    "\n",
    "> Logarithmic Loss.\n",
    "\n",
    "> Area Under ROC Curve.\n",
    "\n",
    "> Confusion Matrix. (http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)\n",
    "\n",
    "> Classification Report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "> Classification Accuracy.\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LogisticRegression()\n",
    "\n",
    "scoring = ' accuracy '\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"Accuracy: %.3f (%.3f)\") % (results.mean(), results.std())\n",
    "\n",
    "> Logarithmic Loss.\n",
    "scoring = ' neg_log_loss '\n",
    "\n",
    "> Area Under ROC Curve:\n",
    "scoring = ' roc_auc '\n",
    "\n",
    "Sensitivity is the true positive rate also called the recall. It is the number of instances\n",
    "from the positive (first) class that actually predicted correctly.\n",
    "\n",
    "Specificity is also called the true negative rate. Is the number of instances from the\n",
    "negative (second) class that were actually predicted correctly.\n",
    "\n",
    "ex: If AUC is 0.8 - relatively close to 1 and greater than 0.5, suggesting some skill in the predictions\n",
    "\n",
    "> Confusion Matrix.\n",
    "\n",
    "The table presents predictions on the x-axis and accuracy outcomes on the y-axis.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\n",
    "random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "matrix = confusion_matrix(Y_test, predicted)\n",
    "\n",
    "\n",
    "> Classification Report.\n",
    "\n",
    "Quick idea of the accuracy of a model using a number of measure like precision, recall, F1-score and support for each\n",
    "class.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\n",
    "random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "report = classification_report(Y_test, predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Regression Metrics:\n",
    "    \n",
    "> Mean Absolute Error.\n",
    "\n",
    "> Mean Squared Error.\n",
    "\n",
    "> R squared/coefficient of determination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "> Mean Absolute Error.\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LinearRegression()\n",
    "scoring = ' neg_mean_absolute_error '\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\n",
    "A value of 0 indicates no error or perfect predictions\n",
    "\n",
    "> Mean Squared Error.\n",
    "scoring = ' neg_mean_squared_error '\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\n",
    "> R squared/coefficient of determination.\n",
    "scoring = ' r2 '\n",
    "\n",
    "This is a value between 0 and 1 for no-fit and perfect fit respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Six classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Linear learning algorithms:\n",
    "    \n",
    "1. Logistic Regression.\n",
    "2. Linear Discriminant Analysis.\n",
    "\n",
    "Nonlinear learning algorithms:\n",
    "3. k-Nearest Neighbors.\n",
    "4. Naive Bayes.\n",
    "5. Classification and Regression Trees.\n",
    "6. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 8. Linear regression vs Logistic regression\n",
    "\n",
    "In linear regression, the outcome (dependent variable) is continuous. It can have any one of an infinite number of possible values. \n",
    "\n",
    "In logistic regression, the outcome (dependent variable) has only a limited number of possible values.\n",
    "\n",
    "For instance, if X contains the area in square feet of houses, and Y contains the corresponding sale price of those houses, you could use linear regression to predict selling price as a function of house size. While the possible selling price may not actually be any, there are so many possible values that a linear regression model would be chosen.\n",
    "\n",
    "If, instead, you wanted to predict, based on size, whether a house would sell for more than 200K, you would use logistic regression. The possible outputs are either Yes, the house will sell for more than 200K, or No, the house will not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does linear mean?\n",
    "\n",
    "Linear regression requires a linear model. No surprise, right? But what does that really mean?\n",
    "\n",
    "A model is linear when each term is either a constant or the product of a parameter and a predictor variable. A linear equation is constructed by adding the results for each term. This constrains the equation to just one basic form:\n",
    "\n",
    "Response = constant + parameter * predictor + ... + parameter * predictor\n",
    "\n",
    "Y = b o + b1X1 + b2X2 + ... + bkXk\n",
    "\n",
    "While a linear equation has one basic form, nonlinear equations can take many different forms.\n",
    "Literally, itâ€™s not linear. If the equation doesnâ€™t meet the criteria above for a linear equation, itâ€™s nonlinear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 9. Regression Algorithms:\n",
    "#### Linear machine learning algorithms:\n",
    "1. Linear Regression.\n",
    "2. Ridge Regression.\n",
    "3. LASSO Linear Regression.\n",
    "4. Elastic Net Regression.\n",
    "\n",
    "#### Nonlinear machine learning algorithms:\n",
    "5. k-Nearest Neighbors.\n",
    "6. Classification and Regression Trees.\n",
    "7. Support Vector Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 10. Classification algorithms:\n",
    "\n",
    "#### Linear algorithms\n",
    "1. Logistic Regression \n",
    "2. Linear Discriminant Analysis\n",
    "   \n",
    "#### Nonlinear algorithms:\n",
    "3. k-Nearest Neighbors\n",
    "4. Naive Bayes\n",
    "5. Classification and Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_svm=SVC()\n",
    "results_svm=cross_val_score(model_svm,X,Y,cv=kfold)\n",
    "print (\"SVM: %s\" % results_svm.mean())\n",
    "SVM: 0.651025290499\n",
    "\n",
    "model_knn = KNeighborsClassifier()\n",
    "results_knn = cross_val_score(model_knn,X,Y,cv=kfold)\n",
    "print (\"KNN: %s\" % results_knn.mean())\n",
    "KNN: 0.726555023923\n",
    "\n",
    "\n",
    "model_decision = DecisionTreeClassifier()\n",
    "results_decision = cross_val_score(model_decision,X,Y,cv=kfold)\n",
    "print (\"Descision Tree: %s\" % results_decision.mean())\n",
    "Descision Tree: 0.691285030759\n",
    "\n",
    "    \n",
    "model_LDA = LinearDiscriminantAnalysis()\n",
    "results_model_LDA = cross_val_score(model_LDA,X,Y,cv=kfold)\n",
    "print (\"LDA: %s\" % results_model_LDA.mean())\n",
    "LDA: 0.773462064252\n",
    "\n",
    "model_LogisticRegression = LogisticRegression()\n",
    "results_LogisticRegression = cross_val_score(model_LogisticRegression,X,Y,cv=kfold)\n",
    "print (\"Logistic Regression: %s\" % results_LogisticRegression.mean())\n",
    "Logistic Regression: 0.76951469583\n",
    "\n",
    "    \n",
    "model_decision = DecisionTreeClassifier()\n",
    "results_decision = cross_val_score(model_decision,X,Y,cv=kfold)\n",
    "print (\"Descision Tree: %s\" % results_decision.mean())\n",
    "Descision Tree: 0.699213943951"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 11. Compare Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare models\n",
    "models = []\n",
    "models.append(( ' LR ' , LogisticRegression()))\n",
    "models.append(( ' LDA ' , LinearDiscriminantAnalysis()))\n",
    "models.append(( ' KNN ' , KNeighborsClassifier()))\n",
    "models.append(( ' CART ' , DecisionTreeClassifier()))\n",
    "models.append(( ' NB ' , GaussianNB()))\n",
    "models.append(( ' SVM ' , SVC()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = ' accuracy '\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=7)\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print (msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 12. Pipelines - Data Preparation and Modeling Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines help you prevent data leakage in your test harness by ensuring that data preparation\n",
    "like standardization is constrained to each fold of your cross validation procedure.\n",
    "\n",
    "Note:\n",
    "Data preparation is one easy way to leak knowledge of the whole training dataset to the algorithm. For example, preparing your data using normalization or standardization on the entire training dataset before learning would not\n",
    "be a valid test because the training dataset would have been influenced by the scale of the data in the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(( ' standardize ' , StandardScaler()))\n",
    "estimators.append(( ' lda ' , LinearDiscriminantAnalysis()))\n",
    "model = Pipeline(estimators)\n",
    "# evaluate pipeline\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Feature Extraction and Modeling Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction is another procedure that is susceptible to data leakage. Like data preparation,\n",
    "feature extraction procedures must be restricted to the data in your training dataset. The\n",
    "pipeline provides a handy tool called the FeatureUnion which allows the results of multiple\n",
    "feature selection and extraction procedures to be combined into a larger dataset on which a\n",
    "model can be trained. Importantly, all the feature extraction and the feature union occurs\n",
    "within each fold of the cross validation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create feature union\n",
    "features = []\n",
    "features.append(( ' pca ' , PCA(n_components=3)))\n",
    "features.append(( ' select_best ' , SelectKBest(k=6)))\n",
    "feature_union = FeatureUnion(features)\n",
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(( ' feature_union ' , feature_union))\n",
    "estimators.append(( ' logistic ' , LogisticRegression()))\n",
    "model = Pipeline(estimators)\n",
    "# evaluate pipeline\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 14. Ensembles means combining models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 14.1 Bootstrap Aggregation (or Bagging)\n",
    "\n",
    "Bootstrap Aggregation (or Bagging) involves taking multiple samples from your training dataset\n",
    "(with replacement) and training a model for each sample. The final output prediction is averaged\n",
    "across the predictions of all of the sub-models. The three bagging models covered in this section\n",
    "are as follows:\n",
    ">  Bagged Decision Trees.\n",
    " \n",
    "> Random Forest.\n",
    "\n",
    "> Extra Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Bagged Decision Trees\n",
    "Bagging performs best with algorithms that have high variance. A popular example are\n",
    "decision trees, often constructed without pruning. In the example below is an example\n",
    "of using the BaggingClassifier with the Classification and Regression Trees algorithm\n",
    "(DecisionTreeClassifier 1 ). A total of 100 trees are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "Random Forests is an extension of bagged decision trees. Samples of the training dataset are\n",
    "taken with replacement, but the trees are constructed in a way that reduces the correlation\n",
    "between individual classifiers. Specifically, rather than greedily choosing the best split point in\n",
    "the construction of each tree, only a random subset of features are considered for each split. You\n",
    "can construct a Random Forest model for classification using the RandomForestClassifier\n",
    "class 2 . The example below demonstrates using Random Forest for classification with 100 trees\n",
    "and split points chosen from a random selection of 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_trees = 100\n",
    "max_features = 3\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Trees\n",
    "Extra Trees are another modification of bagging where random trees are constructed from\n",
    "samples of the training dataset. You can construct an Extra Trees model for classification using\n",
    "the ExtraTreesClassifier class 3 . The example below provides a demonstration of extra trees\n",
    "with the number of trees set to 100 and splits chosen from 7 random features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_trees = 100\n",
    "max_features = 7\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2 Boosting Algorithms\n",
    "Boosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes\n",
    "of the models before them in the sequence. Once created, the models make predictions which\n",
    "may be weighted by their demonstrated accuracy and the results are combined to create a final\n",
    "output prediction. The two most common boosting ensemble machine learning algorithms are:\n",
    "> AdaBoost.\n",
    "\n",
    "> Stochastic Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost \n",
    "AdaBoost generally works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay or less attention to them in the construction of subsequent models. You\n",
    "can construct an AdaBoost model for classification using the AdaBoostClassifier class 4 . The\n",
    "example below demonstrates the construction of 30 decision trees in sequence using the AdaBoost\n",
    "algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_trees = 30\n",
    "seed=7\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Boosting\n",
    "Stochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most\n",
    "sophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of\n",
    "the best techniques available for improving performance via ensembles. You can construct a\n",
    "Gradient Boosting model for classification using the GradientBoostingClassifier class 5 . The\n",
    "example below demonstrates Stochastic Gradient Boosting for classification with 100 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_trees = 100\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Ensemble\n",
    "Voting is one of the simplest ways of combining the predictions from multiple machine learning\n",
    "algorithms. It works by first creating two or more standalone models from your training dataset.\n",
    "A Voting Classifier can then be used to wrap your models and average the predictions of the\n",
    "sub-models when asked to make predictions for new data.\n",
    "\n",
    "You can create a voting ensemble model for classification using the VotingClassifier\n",
    "class. The code below provides an example of combining the predictions of logistic regression,\n",
    "classification and regression trees and support vector machines together for a classification\n",
    "problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(( ' logistic ' , model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(( ' cart ' , model2))\n",
    "model3 = SVC()\n",
    "estimators.append(( ' svm ' , model3))\n",
    "\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "\n",
    "results = cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Improve Performance with Algorithm Tuning\n",
    "Algorithm tuning is a final step in the process of applied machine learning before finalizing your\n",
    "model. It is sometimes called hyperparameter optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1 Grid Search Parameter Tuning\n",
    "\n",
    "Grid search is an approach to parameter tuning that will methodically build and evaluate a\n",
    "model for each combination of algorithm parameters specified in a grid. You can perform a grid search using the GridSearchCV class. The example below evaluates different alpha values for\n",
    "the Ridge Regression algorithm on the standard diabetes dataset. This is a one-dimensional\n",
    "grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grid Search for Algorithm Tuning\n",
    "alphas = numpy.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "param_grid = dict(alpha=alphas)\n",
    "\n",
    "model = Ridge()\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "\n",
    "grid.fit(X, Y)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2 Random Search Parameter Tuning\n",
    "\n",
    "Random search is an approach to parameter tuning that will sample algorithm parameters from\n",
    "a random distribution (i.e. uniform) for a fixed number of iterations. A model is constructed\n",
    "and evaluated for each combination of parameters chosen. You can perform a random search\n",
    "for algorithm parameters using the RandomizedSearchCV class 2 . The example below evaluates\n",
    "different random alpha values between 0 and 1 for the Ridge Regression algorithm on the\n",
    "standard diabetes dataset. A total of 100 iterations are performed with uniformly random alpha\n",
    "values selected in the range between 0 and 1 (the range that alpha values can take)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Randomized for Algorithm Tuning\n",
    "param_grid = { ' alpha ' : uniform()}\n",
    "model = Ridge()\n",
    "\n",
    "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100,\n",
    "random_state=7)\n",
    "\n",
    "rsearch.fit(X, Y)\n",
    "print(rsearch.best_score_)\n",
    "print(rsearch.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Save and Load Machine Learning Models\n",
    "\n",
    "> ### Pickle\n",
    "\n",
    "> ### Joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize Your Model with pickle\n",
    "Pickle is the standard way of serializing objects in Python. You can use the pickle 1 operation\n",
    "to serialize your machine learning algorithms and save the serialized format to a file. Later you\n",
    "can load this file to deserialize your model and use it to make new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Model Using Pickle\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "filename = ' pima-indians-diabetes.data.csv '\n",
    "names = [ ' preg ' , ' plas ' , ' pres ' , ' skin ' , ' test ' , ' mass ' , ' pedi ' , ' age ' , ' class ' ]\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "# Fit the model on 33%\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = ' finalized_model.sav '\n",
    "dump(model, open(filename, ' wb ' ))\n",
    "\n",
    "# some time later...\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = load(open(filename, ' rb ' ))\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize Your Model with Joblib\n",
    "The Joblib API for efficiently serializing Python objects with NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.joblib import dump\n",
    "from sklearn.externals.joblib import load\n",
    "\n",
    "..........\n",
    "\n",
    "# save the model to disk\n",
    "filename = ' finalized_model.sav '\n",
    "dump(model, filename)\n",
    "\n",
    "# some time later...\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = load(filename)\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
